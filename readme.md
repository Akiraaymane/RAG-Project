ğŸ“˜ RAG Project â€“ Retrieval Augmented Generation (Feature Selection Articles)
ğŸ§  Objectif du projet

Ce projet implÃ©mente un pipeline complet de Retrieval-Augmented Generation (RAG) permettant dâ€™interroger efficacement un corpus dâ€™articles scientifiques portant sur le Feature Selection (sÃ©lection de variables en Machine Learning).

Nous avons construit :

un pipeline dâ€™indexation avec FAISS

un moteur de recherche sÃ©mantique

un module de gÃ©nÃ©ration basÃ© sur un LLM

un systÃ¨me dâ€™Ã©valuation avancÃ© (classique + LLM-as-a-Judge)

une interface en ligne de commande (CLI)

Le tout dans une architecture propre et modulaire.

ğŸ“‚ DonnÃ©es utilisÃ©es

Nous avons travaillÃ© sur 4 articles scientifiques traitant des techniques de Feature Selection, incluant :

Filter methods

Wrapper methods

Embedded methods

Mutual Information, SFS, SFFS

Hybrid & Ensemble feature selection

Ã‰tudes rÃ©centes (Cai et al., Khan et al., etc.)

Ces documents sont placÃ©s dans :

data/
    Features_selection_1.pdf
    Features_selection_2.pdf
    Features_selection_3.pdf
    Features_selection_4.pdf

ğŸ—ï¸ Architecture technique
ğŸ”¹ FAISS

UtilisÃ© comme vector store pour lâ€™indexation efficace des embeddings.
FAISS est rapide, optimisÃ© et standard dans les pipelines RAG.

ğŸ”¹ LangChain

Framework permettant de gÃ©rer :

les loaders

les chunkers

les embeddings

les vector stores

les pipelines de recherche

ğŸ”¹ Embeddings â€“ all-MiniLM-L6-v2

Nous avons choisi le modÃ¨le sentence-transformers/all-MiniLM-L6-v2 pour trois raisons :

TrÃ¨s bonne qualitÃ© des reprÃ©sentations sÃ©mantiques dans les tÃ¢ches QA.

Faible coÃ»t computationnel (modÃ¨le lÃ©ger â†’ rapide pour FAISS).

RecommandÃ© dans les systÃ¨mes RAG pour des documents courts/moyens.

ğŸ”¹ LLM (Groq API)

Pour la gÃ©nÃ©ration finale et lâ€™Ã©valuation LLM-as-a-Judge, nous utilisons un modÃ¨le LLama 3.1 via lâ€™API Groq (infÃ©rence ultra rapide).

âš™ï¸ FonctionnalitÃ©s du projet
âœ” 1) Indexation des documents

Extraction PDF (PyPDF)

Chunking (500â€“800 tokens)

Embedding MiniLM

Stockage FAISS (persistant)

âœ” 2) Recherche sÃ©mantique

SimilaritÃ© cosine

RÃ©cupÃ©ration des top-k chunks

Score FAISS + mÃ©tadonnÃ©es (page, source)

âœ” 3) GÃ©nÃ©ration de rÃ©ponse (RAG)

Contexte = top-k chunks

Prompt structurÃ© â€œquestion + contexteâ€

ModÃ¨le Groq (LLama 3.1)

âœ” 4) Ã‰valuation complÃ¨te

Nous avons Ã©valuÃ© le systÃ¨me sur 15 questions (Human feedback).

ğŸ”¹ Ã‰valuation du retrieval (retrieval quality)

Sur les 15 questions :

Recall@4 = 0.867
â†’ dans 86.7% des cas, au moins un chunk pertinent est prÃ©sent dans les 4 premiers rÃ©sultats.

Precision@4 = 0.450
â†’ en moyenne, 45% des chunks retournÃ©s sont rÃ©ellement pertinents.

â¡ï¸ InterprÃ©tation :
Le pipeline de rÃ©cupÃ©ration est trÃ¨s bon (haut recall), mais retourne parfois un peu de bruit (precision moyenne).
Ce comportement est attendu avec MiniLM (embedding lÃ©ger).

ğŸ“Š Ã‰valuation des rÃ©ponses gÃ©nÃ©rÃ©es

Nous Ã©valuons la qualitÃ© des rÃ©ponses selon 4 mÃ©triques :

ğŸ”¹ 1) ROUGE-L : 0.28

ROUGE compare la similaritÃ© entre rÃ©ponse gÃ©nÃ©rÃ©e et rÃ©fÃ©rence humaine.

â†’ Score modÃ©rÃ© : acceptable pour un LLM utilisant un contexte chunkÃ©.

ğŸ”¹ 2) Cosine Similarity (embeddings) : 0.70

Similitude sÃ©mantique entre la rÃ©ponse gÃ©nÃ©rÃ©e et la rÃ©ponse idÃ©ale.
â†’ 0.70 indique que la rÃ©ponse est globalement dans le bon sujet.

ğŸ§  Ã‰valuation LLM-as-a-Judge (Groq)

Nous avons ajoutÃ© deux mÃ©triques avancÃ©es, essentielles en RAG :

ğŸ”¹ 3) Faithfulness (FidÃ©litÃ© au contexte) : 0.77

Mesure :

Est-ce que le modÃ¨le invente des informations non prÃ©sentes dans les chunks ?

CalculÃ© avec Llama 3.1 (Groq).
Un score de 0.77 indique trÃ¨s peu dâ€™hallucinations.

ğŸ”¹ 4) Answer Relevance (Pertinence de la rÃ©ponse) : 0.60

Mesure :

Est-ce que la rÃ©ponse rÃ©pond vraiment Ã  la question ?

Score correct, mais montre que certaines rÃ©ponses sont :

trop gÃ©nÃ©rales

trop courtes

ou sâ€™Ã©loignent lÃ©gÃ¨rement de lâ€™intention de la question

ğŸ¯ Pourquoi ces mÃ©triques ?
MÃ©trique	Pourquoi ?	RÃ´le
Recall@k	VÃ©rifie si on rÃ©cupÃ¨re la bonne info	QualitÃ© du retrieval
Precision@k	VÃ©rifie si le contexte est propre	Bruit dans FAISS
ROUGE-L	Compare rÃ©ponse vs rÃ©fÃ©rence	Surface-level correctness
Cosine similarity	VÃ©rifie la proximitÃ© sÃ©mantique	Deep meaning correctness
Faithfulness (LLM judge)	VÃ©rifie les hallucinations	FiabilitÃ©
Answer Relevance (LLM judge)	VÃ©rifie lâ€™adÃ©quation	Pertinence rÃ©elle

â¡ï¸ Ensemble, ces mÃ©triques donnent une vision complÃ¨te du RAG (retrieval + generation).

ğŸ–¥ï¸ CLI (Command Line Interface)

Un script cli.py permet d'utiliser le systÃ¨me depuis le terminal :

ğŸ“Œ Indexer :
python cli.py index --config config.yaml

ğŸ“Œ Poser une question :
python cli.py ask --config config.yaml --question "What is SFS?"

ğŸ“Œ Ã‰valuer :
python cli.py evaluate --config config.yaml --k 4 --output results.json

ğŸ“¦ Installation
pip install -r requirements.txt


CrÃ©er la variable dâ€™environnement :

export GROQ_API_KEY="VOTRE_CLÃ‰"

ğŸ§® Arborescence du projet
RAG-Project/
    cli.py
    config.yaml
    data/
    storage/
    src/
        indexer.py
        search.py
        rag.py
        evaluator.py
        llm_judge.py
    results/
    README.md

ğŸ Conclusion

Ce projet met en place un vrai pipeline RAG complet, Ã©valuÃ©, fiable et bien structurÃ©.
Nous avons construit :

ğŸ” un bon retrieval (R@4 = 0.867)

ğŸ§  un modÃ¨le gÃ©nÃ©ratif cohÃ©rent

ğŸ“Š une Ã©valuation avancÃ©e (classique + LLM judge)

ğŸ› ï¸ un outil CLI professionnel

Tu peux lâ€™utiliser comme base pour :

des projets industriels

des chatbots documentaires

de la recherche appliquÃ©e

des systÃ¨mes QA avancÃ©s
